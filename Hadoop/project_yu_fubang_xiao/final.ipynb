{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import nltk\n",
    "from pyspark import SparkContext\n",
    "#from nltk.stem import SnowballStemmer\n",
    "#from tokenize import tokenize\n",
    "#from nltk.corpus import stopwords,words\n",
    "from scipy.spatial.distance import cdist\n",
    "import pronouncing\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import col, split, explode, udf\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# First step\n",
    "# Find all the candidate lines of lyrics that have the same rhyme and sentence structure with the given line\n",
    "# Input: csv file\n",
    "# Output: dataframe of all the possible candidate lines\n",
    "# Using library pronouncing to handle with the rhyme\n",
    "# Count the length of a sentence to make sure each line in rap is almost the same length\n",
    "\n",
    "rowData = spark.read.csv(\"/FileStore/tables/lyrics.csv\", inferSchema=True, header = True, multiLine=True)\n",
    "rowData = rowData.filter(rowData.genre == 'Hip-Hop')\n",
    "rowData.show(n=5)\n",
    "data_array_lyrics = rowData.withColumn(\n",
    "    \"lyrics\",\n",
    "    split(col(\"lyrics\"), \"\\n\").cast(ArrayType(StringType())).alias(\"lyrics\")\n",
    ")\n",
    "data_array_lyrics.show(n=5)\n",
    "data_line = data_array_lyrics.withColumn(\"lyrics\", explode(data_array_lyrics.lyrics))\n",
    "data_line.show(n=5)\n",
    "def phone_phrase(phrase):\n",
    "    phone_consonant = ['B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K', 'L', 'M', 'N', 'P', 'R', 'S', 'SH', 'T', 'TH', 'V', 'W', 'Y', 'Z', 'ZH']\n",
    "    s = filter(bool, re.split(r'\\W+', phrase.lower()))\n",
    "    phone = []\n",
    "    for p in s:\n",
    "    tmp = pronouncing.phones_for_word(p)\n",
    "    if len(tmp) != 0:\n",
    "        tmp_list = tmp[0].split()\n",
    "        for x in tmp_list:\n",
    "            if x not in phone_consonant:\n",
    "                phone.append(x[:-1])\n",
    "    if len(phone) >=3:\n",
    "    res = phone[-3] + phone[-2] + phone[-1]\n",
    "    else:\n",
    "    res = ''\n",
    "    return res\n",
    "pp_udf = udf(phone_phrase, StringType())\n",
    "\n",
    "data_phone = data_line.withColumn(\"phoneme\", pp_udf(data_line['lyrics']))\n",
    "data_phone.show(n=20)\n",
    "def length_phrase(phrase):\n",
    "    s = filter(bool, re.split(r'\\W+', phrase.lower()))\n",
    "    return len(s)\n",
    "len_udf = udf(length_phrase, IntegerType())\n",
    "data_phone_len = data_phone.withColumn('length', len_udf(data_phone['lyrics']))\n",
    "data_phone_len.show(n=20)\n",
    "data_phone_len.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_lyric = 'Today I would like show a freestyle rap lyrics'\n",
    "pho, length = phone_phrase(test_lyric), length_phrase(test_lyric)\n",
    "print pho, length\n",
    "print type(pho)\n",
    "data_final = data_phone_len.filter(data_phone_len.phoneme == pho).filter(data_phone_len.length<length+5).filter(data_phone_len.length>length-5)\n",
    "data_final = data_final.select(data_final.lyrics).distinct()\n",
    "data_final.show(n=100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second step: preprocessing\n",
    "# Given a set of candidate lines, preprocess those lines and do some feature extractin\n",
    "# Input: dataframe of candidate lines\n",
    "# Output: rdd of 2 grams\n",
    "\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('NLP_starter').getOrCreate()\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer = Tokenizer(inputCol='lyrics', outputCol='words')\n",
    "count_words = udf(lambda words: len(words), IntegerType())\n",
    "# tokenized_df = tokenizer.transform(df)\n",
    "\n",
    "regex_tokenizer = RegexTokenizer(inputCol='lyrics', outputCol='words', pattern='\\\\W')\n",
    "#regex_tokenizer.setMinTokenLength(4)\n",
    "regex_df = regex_tokenizer.transform(df)\n",
    "regex_tokenized_counts = regex_df.withColumn('freq', count_words('words'))\n",
    "regex_tokenized_counts.show(truncate=False)\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='tokens')\n",
    "stopWords = ['a','an','the', 'is','are', 'for', 'hi', 'in', 'on','row','lyrics','u','t','s','re','i','m']\n",
    "remover.setStopWords(stopWords)\n",
    "tokens_filtered = remover.transform(regex_tokenized_counts)\n",
    "cleanDF= tokens_filtered.withColumn('count_tokens', count_words('tokens'))\n",
    "\n",
    "#tokens_filtered1 = remover.transform(newcleanDF)\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram(n=2, inputCol='tokens', outputCol='2grams')\n",
    "my_2ngrams =ngram.transform(cleanDF)\n",
    "my_2ngrams.show()\n",
    "my_2ngrams.select('2grams').show(truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types = [f.dataType for f in my_2ngrams.schema.fields]\n",
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grams = my_2ngrams.select('2grams').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gramlist = [list(g[0]) for g in grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gramrdd = sc.parallelize(gramlist)\n",
    "gramrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf8\n",
    "from pyspark import SparkContext\n",
    "from pyspark import RDD\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import sys\n",
    "if sys.version[0] == '2':\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "class PLSA:\n",
    "\n",
    "    def __init__(self, data, sc, k, is_test=False, max_itr=1000, eta=1e-6):\n",
    "\n",
    "        \"\"\"\n",
    "        init the algorithm\n",
    "\n",
    "        :type data RDD\n",
    "        :param data: document rdd\n",
    "        :type max_itr int\n",
    "        :param max_itr: maximum EM iter\n",
    "        :type is_test bool\n",
    "        :param is_test: test or not,if yes, rd = RandomState(1)，otherwise rd = RandomState()\n",
    "        :type sc SparkContext\n",
    "        :param sc: spark context\n",
    "        :type k int\n",
    "        :param k : number of theme\n",
    "        :type eta float\n",
    "        :param : threshold，when the changement of log likelyhood<eta, stop iteration\n",
    "        :return : PLSA object\n",
    "        \"\"\"\n",
    "\n",
    "        self.max_itr = max_itr\n",
    "        self.k = sc.broadcast(k)\n",
    "        self.ori_data = data#.map(lambda x: x.split(' '))\n",
    "        self.data = data\n",
    "        self.sc = sc\n",
    "        self.eta = eta\n",
    "        self.rd = sc.broadcast(RandomState(1) if is_test else RandomState())\n",
    "\n",
    "    def train(self):\n",
    "        #get the dictionary words\n",
    "        self.word_dict_b = self._init_dict_()\n",
    "        #transform the words in the documents into the indexes in the dictionary\n",
    "        self._convert_docs_to_word_index()\n",
    "        #initialization, the distribution under each theme\n",
    "        self._init_probility_word_topic_()\n",
    "\n",
    "        pre_l= self._log_likelyhood_()\n",
    "\n",
    "        print(\"L(%d)=%.5f\" %(0,pre_l))\n",
    "\n",
    "        for i in range(self.max_itr):\n",
    "            #update the posterior distribution\n",
    "            self._E_step_()\n",
    "            #maimize the lower bound\n",
    "            self._M_step_()\n",
    "            now_l = self._log_likelyhood_()\n",
    "\n",
    "            improve = np.abs((pre_l-now_l)/pre_l)\n",
    "            pre_l = now_l\n",
    "\n",
    "            print(\"L(%d)=%.5f with %.6f%% improvement\" %(i+1,now_l,improve*100))\n",
    "            if improve <self.eta:\n",
    "                break\n",
    "\n",
    "    def _M_step_(self):\n",
    "        \"\"\"\n",
    "        update: p(z=k|d),p(w|z=k)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        k = self.k\n",
    "        v = self.v\n",
    "\n",
    "        def update_probility_of_doc_topic(doc):\n",
    "            \"\"\"\n",
    "            update the distribution of the documents of the themes\n",
    "            \"\"\"\n",
    "            topic_doc = doc['topic'] - doc['topic']\n",
    "            words = doc['words']\n",
    "            for (word_index,word) in words.items():\n",
    "                topic_doc += word['count']*word['topic_word']\n",
    "            topic_doc /= np.sum(topic_doc)\n",
    "\n",
    "            return {'words':words,'topic':topic_doc}\n",
    "\n",
    "        self.data = self.data.map(update_probility_of_doc_topic)\n",
    "        \n",
    "        self.data.cache()\n",
    "\n",
    "        def update_probility_word_given_topic(doc):\n",
    "            \"\"\"\n",
    "            up date the distribution of the words of the themes\n",
    "            \"\"\"\n",
    "            probility_word_given_topic = np.matrix(np.zeros((k.value,v.value)))\n",
    "\n",
    "            words = doc['words']\n",
    "            for (word_index,word) in words.items():\n",
    "                probility_word_given_topic[:,word_index] += np.matrix(word['count']*word['topic_word']).T\n",
    "\n",
    "            return probility_word_given_topic\n",
    "\n",
    "        probility_word_given_topic = self.data.map(update_probility_word_given_topic).sum()\n",
    "        probility_word_given_topic_row_sum = np.matrix(np.sum(probility_word_given_topic,axis=1))\n",
    "\n",
    "        #normalization\n",
    "        probility_word_given_topic = np.divide(probility_word_given_topic,probility_word_given_topic_row_sum)\n",
    "\n",
    "        self.probility_word_given_topic = self.sc.broadcast(probility_word_given_topic)\n",
    "\n",
    "    def _E_step_(self):\n",
    "        \"\"\"\n",
    "        update the latent viariable:  p(z|w,d)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        probility_word_given_topic = self.probility_word_given_topic\n",
    "        k = self.k\n",
    "\n",
    "        def update_probility_of_word_topic_given_word(doc):\n",
    "            topic_doc = doc['topic']\n",
    "            words = doc['words']\n",
    "\n",
    "            for (word_index,word) in words.items():\n",
    "                topic_word = word['topic_word']\n",
    "                for i in range(k.value):\n",
    "                    topic_word[i] = probility_word_given_topic.value[i,word_index]*topic_doc[i]\n",
    "                #normalization\n",
    "                topic_word /= np.sum(topic_word)\n",
    "                word['topic_word'] = topic_word # added\n",
    "            return {'words':words,'topic':topic_doc}\n",
    "\n",
    "        self.data = self.data.map(update_probility_of_word_topic_given_word)\n",
    "\n",
    "    def  _init_probility_word_topic_(self):\n",
    "        \"\"\"\n",
    "        init p(w|z=k)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        #dict length(words in dict)\n",
    "        m = self.v.value\n",
    "\n",
    "        probility_word_given_topic = self.rd.value.uniform(0,1,(self.k.value,m))\n",
    "        probility_word_given_topic_row_sum = np.matrix(np.sum(probility_word_given_topic,axis=1)).T\n",
    "\n",
    "        #normalization\n",
    "        probility_word_given_topic = np.divide(probility_word_given_topic,probility_word_given_topic_row_sum)\n",
    "\n",
    "        self.probility_word_given_topic = self.sc.broadcast(probility_word_given_topic)\n",
    "\n",
    "    def _convert_docs_to_word_index(self):\n",
    "\n",
    "        word_dict_b = self.word_dict_b\n",
    "        k = self.k\n",
    "        rd = self.rd\n",
    "        '''\n",
    "        I wonder is there a better way to execute function with broadcast varible\n",
    "        '''\n",
    "        def _word_count_doc_(doc):\n",
    "            print(doc)\n",
    "            wordcount ={}\n",
    "            word_dict = word_dict_b.value\n",
    "            for word in doc:\n",
    "                if word_dict[word] in wordcount:\n",
    "                    wordcount[word_dict[word]]['count'] += 1\n",
    "                else:\n",
    "                    #first one is the number of word occurance\n",
    "                    #second one is p(z=k|w,d)\n",
    "                    wordcount[word_dict[word]] = {'count':1,'topic_word': rd.value.uniform(0,1,k.value)}\n",
    "\n",
    "            topics = rd.value.uniform(0, 1, k.value)\n",
    "            topics = topics/np.sum(topics)\n",
    "            return {'words':wordcount,'topic':topics}\n",
    "        self.data = self.ori_data.map(_word_count_doc_)\n",
    "\n",
    "    def _init_dict_(self):\n",
    "        \"\"\"\n",
    "        init word dict of the documents,\n",
    "        and broadcast it\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        words = self.ori_data.flatMap(lambda d: d).distinct().collect()\n",
    "        word_dict = {w: i for w, i in zip(words, range(len(words)))}\n",
    "        self.v = self.sc.broadcast(len(word_dict))\n",
    "        return self.sc.broadcast(word_dict)\n",
    "\n",
    "    def _log_likelyhood_(self):\n",
    "        \n",
    "        probility_word_given_topic = self.probility_word_given_topic\n",
    "        k = self.k\n",
    "        def likelyhood(doc):\n",
    "            print(\"succ\")\n",
    "            l = 0.0\n",
    "            topic_doc = doc['topic']\n",
    "            words = doc['words']\n",
    "            for (word_index,word) in words.items():\n",
    "                print(word)\n",
    "                l += word['count']*np.log(np.matrix(topic_doc)*probility_word_given_topic.value[:,word_index])\n",
    "            return l\n",
    "        return self.data.map(likelyhood).sum()\n",
    "\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        save the result of the model TODO \n",
    "        :param f_word_given_topic: distribution of words given the topic\n",
    "        :param f_doc_topic:  distribution of topic given the documents\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        doc_topic = self.data.map(lambda x:' '.join([str(q) for q in x['topic'].tolist()])).collect()\n",
    "        probility_word_given_topic = self.probility_word_given_topic.value\n",
    "\n",
    "        word_dict = self.word_dict_b.value\n",
    "        word_given_topic = []\n",
    "\n",
    "        for w,i in word_dict.items():\n",
    "            word_given_topic.append('%s %s' %(w,' '.join([str(q[0]) for q in probility_word_given_topic[:,i].tolist()])))\n",
    "        return word_given_topic, doc_topic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plsa = PLSA(data=gramrdd,sc=sc,k=5,max_itr=10,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plsa.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_given_topic, topic_given_doc = plsa.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame([sub.split(\" \") for sub in word_given_topic])\n",
    "topic_word_1 = topic_word[topic_word.columns[:6]]\n",
    "topic_word_1 = topic_word_1.set_index(0).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic = pd.DataFrame([sub.split(\" \") for sub in topic_given_doc])\n",
    "doc_topic_1 = doc_topic[doc_topic.columns[:6]]\n",
    "doc_topic_1 = doc_topic_1.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_word.sort_values(topic_word.columns[5],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_given_doc_1 = [x.split(\" \") for x in topic_given_doc]\n",
    "topic_given_doc_2 = [[float(y) for y in x]for x in topic_given_doc_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "some_pt = topic_given_doc_2[0]\n",
    "min_index = distance.cdist([some_pt], topic_given_doc_2)[0].argsort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(min_index[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfl = df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print test_lyric\n",
    "for i in min_index[1:6]:\n",
    "    print(dfl[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "PLSA_topic_mining",
  "notebookId": 2974578033065903
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
